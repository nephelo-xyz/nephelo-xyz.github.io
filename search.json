[{"title":"Kubernetes本地集群搭建","url":"/2020/04/20/Kubernetes本地集群搭建/","content":"\n> 最近在着手搭建Kubernetes的集群，确实还挺多问题，所以整理出来记录一下。\n\n<!-- more -->\n\n## 1 版本统一\n\n- Docker 18.09.0\n- kubeadm-1.14.0-0 \n- kubelet-1.14.0-0 \n- kubectl-1.14.0-0\n  - k8s.gcr.io/kube-apiserver:v1.14.0\n  - k8s.gcr.io/kube-controller-manager:v1.14.0\n  - k8s.gcr.io/kube-scheduler:v1.14.0\n  - k8s.gcr.io/kube-proxy:v1.14.0\n  - k8s.gcr.io/pause:3.1\n  - k8s.gcr.io/etcd:3.3.10\n  - k8s.gcr.io/coredns:1.3.1\n- calico:v3.9\n\n## 2 准备3台centos\n\n```shell\n01 `搭建前准备`\nmkdir D:\\VM\\k8s-docker-centos7 【创建目录】\ncd D:\\VM\\k8s-docker-centos7 【切换目录】\ntype nul>Vagrantfile 【创建Vagrantfile文件】\necho [fileContent]>Vagrantfile 【编辑文件】\n# ====================================================================================\nboxes = [\n\t{\n\t\t:name => \"master-kubeadm-k8s\",\n\t\t:eth1 => \"192.168.31.100\",\n\t\t:mem => \"2048\",\n\t\t:cpu => \"2\",\n\t\t:sshport => 22230\n\t},\n\t{\n\t\t:name => \"worker01-kubeadm-k8s\",\n\t\t:eth1 => \"192.168.31.101\",\n\t\t:mem => \"2048\",\n\t\t:cpu => \"2\",\n\t\t:sshport => 22231\n\t},\n\t{\n\t\t:name => \"worker02-kubeadm-k8s\",\n\t\t:eth1 => \"192.168.31.102\",\n\t\t:mem => \"2048\",\n\t\t:cpu => \"2\",\n\t\t:sshport => 22232\n\t}\n]\nVagrant.configure(2) do |config|\n\tconfig.vm.box = \"centos/7\"\n\tboxes.each do |opts|\n\t\tconfig.vm.define opts[:name] do |config|\n\t\t\tconfig.vm.hostname = opts[:name]\n\t\t\tconfig.vm.network :public_network, ip: opts[:eth1]\n\t\t\tconfig.vm.network \"forwarded_port\", guest: 22, host: 2222, id: \"ssh\", disabled: \"true\"\n\t\tconfig.vm.network \"forwarded_port\", guest: 22, host: opts[:sshport]\n\t\t\tconfig.vm.provider \"vmware_fusion\" do |v|\n\t\t\t\tv.vmx[\"memsize\"] = opts[:mem]\n\t\t\t\tv.vmx[\"numvcpus\"] = opts[:cpu]\n\t\t\tend\n\t\t\tconfig.vm.provider \"virtualbox\" do |v|\n\t\t\t\tv.customize [\"modifyvm\", :id, \"--memory\", opts[:mem]]\n\t\t\tv.customize [\"modifyvm\", :id, \"--cpus\", opts[:cpu]]\n\t\t\t\tv.customize [\"modifyvm\", :id, \"--name\", opts[:name]]\n\t\t\tend\n\t\tend\n\tend\nend\n# ====================================================================================\n02 `启动之后进入到对应的centos里面，使得root账户能够登陆，从而使用XShell登陆`\n\tvagrant up [启动虚拟机]\n\tvagrant ssh master-kubeadm-k8s [进入manager-node]\n\tvagrant ssh worker01-kubeadm-k8s [进入worker01-node]\n\tvagrant ssh worker02-kubeadm-k8s [进入worker02-node]\n03 `分别登陆三个节点,执行下面操作,改成可以密码登陆root账户`\n\tsudo -i [进入root账户]\n\tvi /etc/ssh/sshd_config [设置root账户可以密码登陆]\n\t\t修改PasswordAuthentication yes\n\tpasswd [修改密码]\n\tsystemctl restart sshd [重启sshd服务]\n```\n\n## 3 更新并安装依赖\n\n> 3台机器都需要执行\n\n```shell\nyum -y update\nyum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp\n```\n\n## 4 安装Docker\n\n> 3台机器都需要执行，安装版本为18.09.0\n\n```shell\n01 `进入虚拟机`\n    vagrant ssh [nodeName]\n02 `卸载之前安装的docker`\n    sudo yum remove docker docker latest docker-latest-logrotate \\\n    docker-logrotate docker-engine docker-client docker-client-latest docker-common\n03 `安装必要依赖`\n    sudo yum install -y yum-utils device-mapper-persistent-data lvm2\n04 `添加软件源信息`\n    sudo yum-config-manager \\\n    --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n    yum list | grep docker-ce\n05 `更新yum缓存`\n    sudo yum makecache fast\n06 `安装docker`\n    sudo yum install -y docker-ce-18.09.0 docker-ce-cli-18.09.0 containerd.io [指定安装docker版本]\n07 `启动docker并设置开机启动`\n    sudo systemctl start docker && sudo systemctl enable docker\n08 `测试docker安装是否成功`\n    sudo docker run hello-world\n```\n\n## 5 修改hosts文件\n\n```shell\n01 `master`\n# 设置master的hostname，并且修改hosts文件\n\tsudo hostnamectl set-hostname m\n02 `两个worker`\n# 设置worker01/02的hostname，并且修改hosts文件\n\tsudo hostnamectl set-hostname w1\n\tsudo hostnamectl set-hostname w2\n03 `三台机器`\n\tvi /etc/hosts\n# ====================================================================================\n192.168.31.100 m\n192.168.31.101 w1\n192.168.31.102 w2\n# ====================================================================================\n04 `使用ping测试一下`\n\tping m\n\tping w1\n\tping w2\n```\n\n## 6 系统基础前提配置\n\n```shell\n01 `关闭防火墙`\n\tsystemctl stop firewalld && systemctl disable firewalld\n02 `关闭selinux`\n\tsetenforce 0\n\tsed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n03 `关闭swap`\n\tswapoff -a\n\tsed -i '/swap/s/^\\(.*\\)$/#\\1/g' /etc/fstab\n04 `配置iptables的ACCEPT规则`\n\tiptables -F && iptables -X && iptables \\\n    -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT\n05 `设置系统参数`\n# ====================================================================================\ncat <<EOF >  /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system\n# =======================================================================================\n```\n\n## 7 Installing kubeadm, kubelet and kubectl\n\n```shell\n01 `配置yum源`\n# ====================================================================================\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\n       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n# ====================================================================================\n02 `安装kubeadm&kubelet&kubectl`\n\tyum install -y kubeadm-1.14.0-0 kubelet-1.14.0-0 kubectl-1.14.0-0\n03 `docker和k8s设置同一个cgroup`\n# docker\n\tvi /etc/docker/daemon.json 【文件没内容的话，就新建；有的话，就加上这一句，注意文件的格式[逗号]】\n# ====================================================================================\n{\n\t\"exec-opts\": [\"native.cgroupdriver=systemd\"]\n}\n# ====================================================================================  \n\tsystemctl restart docker 【`重启docker，一定要执行`】\n# kubelet\n\tsed -i \"s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g\" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 【`找不到内容没关系`】\n\tsystemctl enable kubelet && systemctl start kubelet 【`重启kubelet，一定要执行`】\n```\n\n## 8 proxy/pause/scheduler等国内镜像\n\n```shell\n01 `查看kubeadm使用的镜像`\n\tkubeadm config images list\n# ====================================================================================\nk8s.gcr.io/kube-apiserver:v1.14.0\nk8s.gcr.io/kube-controller-manager:v1.14.0\nk8s.gcr.io/kube-scheduler:v1.14.0\nk8s.gcr.io/kube-proxy:v1.14.0\nk8s.gcr.io/pause:3.1\nk8s.gcr.io/etcd:3.3.10\nk8s.gcr.io/coredns:1.3.1\n# ====================================================================================\n02 `解决国外镜像不能访问的问题`\n# 创建kubeadm.sh脚本，用于拉取镜像/打tag/删除原有镜像\n\tvi kubeadm.sh\n# ====================================================================================\n#!/bin/bash\nset -e\nKUBE_VERSION=v1.14.0\nKUBE_PAUSE_VERSION=3.1\nETCD_VERSION=3.3.10\nCORE_DNS_VERSION=1.3.1\nGCR_URL=k8s.gcr.io\nALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers\nimages=(kube-proxy:${KUBE_VERSION}\nkube-scheduler:${KUBE_VERSION}\nkube-controller-manager:${KUBE_VERSION}\nkube-apiserver:${KUBE_VERSION}\npause:${KUBE_PAUSE_VERSION}\netcd:${ETCD_VERSION}\ncoredns:${CORE_DNS_VERSION})\nfor imageName in ${images[@]} ; do\n\tdocker pull $ALIYUN_URL/$imageName\n\tdocker tag  $ALIYUN_URL/$imageName $GCR_URL/$imageName\n\tdocker rmi $ALIYUN_URL/$imageName\ndone\n# ====================================================================================\n03 `运行脚本和查看镜像`\n\tsh ./kubeadm.sh 【运行脚本】\n\tdocker images 【查看镜像】\n04 `将这些镜像推送到自己的阿里云仓库`【可选，根据自己实际的情况】\n\tdocker login --username=happyeveryday2019 registry.cn-hangzhou.aliyuncs.com 【登录自己的阿里云仓库，master节点执行即可】 \n\t密码：******\n\tvi kubeadm-push-aliyun.sh\n# ====================================================================================\n#!/bin/bash\nset -e\nKUBE_VERSION=v1.14.0\nKUBE_PAUSE_VERSION=3.1\nETCD_VERSION=3.3.10\nCORE_DNS_VERSION=1.3.1\nGCR_URL=k8s.gcr.io\nALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/841863085\nimages=(kube-proxy:${KUBE_VERSION}\nkube-scheduler:${KUBE_VERSION}\nkube-controller-manager:${KUBE_VERSION}\nkube-apiserver:${KUBE_VERSION}\npause:${KUBE_PAUSE_VERSION}\netcd:${ETCD_VERSION}\ncoredns:${CORE_DNS_VERSION})\nfor imageName in ${images[@]} ; do\n\tdocker tag $GCR_URL/$imageName $ALIYUN_URL/$imageName\n\tdocker push $ALIYUN_URL/$imageName\n\tdocker rmi $ALIYUN_URL/$imageName\ndone\n# ====================================================================================\n06 `运行脚本`\n\tsh ./kubeadm-push-aliyun.sh\n```\n\n## 9 kube init初始化master\n\n> **官网：** https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/\n\n### 9.1  初始化master节点\n\n```shell\n01 `初始化master节点`\n\tkubeadm reset 【初始化集群状态】\n\tkubeadm init --kubernetes-version=1.14.0 \\\n    --apiserver-advertise-address=192.168.31.100 \\\n    --pod-network-cidr=10.244.0.0/16 【初始化master节点】\n# 注意：记得保存好最后kubeadm join的信息。\n# =======================================================================================\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.31.100:6443 --token fag134.3wot9edrvs82vh6d \\\n    --discovery-token-ca-cert-hash sha256:1df02a06552c02ba0e28e00c80a50e9ff40da81a4cdd53c136a16d3c0233f450\n# =======================================================================================\n02 `根据日志提示执行`\n\tmkdir -p $HOME/.kube\n\tsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n\tsudo chown $(id -u):$(id -g) $HOME/.kube/config\n03 `查看pod`\n\t等待一会儿，同时可以发现像etcd，controller，scheduler等组件都以pod的方式安装成功了\n# 注意：coredns没有启动，需要安装网络插件\n\tkubectl get pods -n kube-system 【查看kube-system的pods】\n\tkubectl get pods --all-namespaces 【查看所有pods】\n# =======================================================================================\nNAME                        READY   STATUS    RESTARTS   AGE\ncoredns-fb8b8dccf-f7g6g     0/1     Pending   0          7m30s\ncoredns-fb8b8dccf-hx765     0/1     Pending   0          7m30s\netcd-m                      1/1     Running   0          6m30s\nkube-apiserver-m            1/1     Running   0          6m36s\nkube-controller-manager-m   1/1     Running   0          6m42s\nkube-proxy-w9m72            1/1     Running   0          7m30s\nkube-scheduler-m            1/1     Running   0          6m24s\n# =======================================================================================\n04 `健康检查`\n\tcurl -k https://localhost:6443/healthz\n# =======================================================================================\n[root@master-kubeadm-k8s ~]# curl -k https://localhost:6443/healthz\nok\n# =======================================================================================\n```\n\n### 9.2 kube init流程 ？？？？？\n\n```shell\n01 `进行一系列检查，以确定这台机器可以部署kubernetes`\n02 `生成kubernetes对外提供服务所需要的各种证书可对应目录`\n\t/etc/kubernetes/pki/*\n03 `为其他组件生成访问kube-ApiServer所需的配置文件`\n\tls /etc/kubernetes/\n    admin.conf  controller-manager.conf  kubelet.conf  scheduler.conf\n04 `为 Master组件生成Pod配置文件`\n    ls /etc/kubernetes/manifests/*.yaml\n    kube-apiserver.yaml \n    kube-controller-manager.yaml\n    kube-scheduler.yaml\n05 `生成etcd的Pod YAML文件`\n    ls /etc/kubernetes/manifests/*.yaml\n    kube-apiserver.yaml \n    kube-controller-manager.yaml\n    kube-scheduler.yaml\n\tetcd.yaml\n06 `一旦这些 YAML文件出现在被 kubelet监视的/etc/kubernetes/manifests/目录下，kubelet就会自动创建这些yaml文件定义的pod，即master组件的容器。master容器启动后，kubeadm会通过检查localhost:443/healthz这个master组件的健康状态检查URL，等待master组件完全运行起来`\n07 `为集群生成一个bootstrap token`\n08 `将ca.crt等Master节点的重要信息，通过ConfigMap的方式保存在etcd中，工后续部署node节点使用`\n09 `最后一步是安装默认插件，kubernetes默认kube-proxy和DNS两个插件是必须安装的`\n```\n\n## 10 部署calico网络插件\n\n```shell\n# 选择网络插件\n\thttps://kubernetes.io/docs/concepts/cluster-administration/addons/\n# calico网络插件\n\thttps://docs.projectcalico.org/v3.9/getting-started/kubernetes/\n# 注意：calico，同样在master节点上操作\n01 `可以先手动pull一下` 【可能拉取较慢】\n\tcurl https://docs.projectcalico.org/v3.9/manifests/calico.yaml | grep image 【版本会变化，需要根   据实际情况拉取镜像】\n# =======================================================================================\n\t      image: calico/cni:v3.9.3\n          image: calico/pod2daemon-flexvol:v3.9.3\n          image: calico/node:v3.9.3\n          image: calico/kube-controllers:v3.9.3\n# =======================================================================================\n\tdocker pull calico/cni:v3.9.3\n    docker pull calico/pod2daemon-flexvol:v3.9.3\n    docker pull calico/node:v3.9.3\n    docker pull calico/kube-controllers:v3.9.3\n    `官方镜像拉取太慢，可以用itcrazy`\n    docker pull registry.cn-hangzhou.aliyuncs.com/itcrazy2016/kube-controllers:v3.9.3\n\tdocker pull registry.cn-hangzhou.aliyuncs.com/itcrazy2016/cni:v3.9.3\n\tdocker pull registry.cn-hangzhou.aliyuncs.com/itcrazy2016/pod2daemon-flexvol:v3.9.3\n\tdocker pull registry.cn-hangzhou.aliyuncs.com/itcrazy2016/node:v3.9.3\n\t`打tag`\n\tdocker tag registry.cn-hangzhou.aliyuncs.com/itcrazy2016/kube-controllers:v3.9.3 \\\n    calico/kube-controllers:v3.9.3\n\tdocker tag registry.cn-hangzhou.aliyuncs.com/itcrazy2016/cni:v3.9.3 \\\n    calico/cni:v3.9.3\n\tdocker tag registry.cn-hangzhou.aliyuncs.com/itcrazy2016/pod2daemon-flexvol:v3.9.3 \\\n    calico/pod2daemon-flexvol:v3.9.3\n\tdocker tag registry.cn-hangzhou.aliyuncs.com/itcrazy2016/node:v3.9.3 \\\n    calico/node:v3.9.3\n\t`删除registry.cn-hangzhou.aliyuncs.com/itcrazy2016/格式的镜像` \n# 注意：打tag不会改变imageId，会删除calico的镜像  \n\tdocker rmi -f $(docker images registry.cn-hangzhou.aliyuncs.com/itcrazy2016/* -aq)\n02 `在k8s中安装calico`\n\tyum install -y wget\n\twget https://docs.projectcalico.org/v3.9/manifests/calico.yaml\n\t`最新版本已经是3.9.5 需要更改为 3.9.3`\n\tsed -i \"s/v3.9.5/v3.9.3/g\" calico.yaml\n\tkubectl apply -f calico.yaml\n03 `确认一下calico是否安装成功`\n\tkubectl get pods --all-namespaces -w 【实时查看所有的Pods】\n04 `删除一个pod 【如有需要的话】`\n\tkubectl delete pod $podName --grace-period=0 --force -n $namespaceName\n```\n\n## 11 kube join\n\n```shell\n01 记得保存初始化master节点的最后打印信息【注意这边大家要自己的，下面我的只是一个参考】\n\tkubeadm join 192.168.31.100:6443 --token fag134.3wot9edrvs82vh6d \\\n    --discovery-token-ca-cert-hash sha256:1df02a06552c02ba0e28e00c80a50e9ff40da81a4cdd53c136a16d3c0233f450【worker上面执行】\n02 在master节点上检查集群信息\n\tkubectl get nodes\n# =======================================================================================\nNAME                   STATUS   ROLES    AGE     VERSION\nmaster-kubeadm-k8s     Ready    master   19m     v1.14.0\nworker01-kubeadm-k8s   Ready    <none>   3m6s    v1.14.0\nworker02-kubeadm-k8s   Ready    <none>   2m41s   v1.14.0\n# =======================================================================================\n```\n\n## 12 再次体验Pod\n\n```shell\n01 `定义pod.yml文件，比如pod_nginx_rs.yaml` 【不能使用tab，只能用空格】\n\tmkdir pod_nginx_rs\n\tcd pod_nginx_rs\n# =======================================================================================\ncat > pod_nginx_rs.yaml <<EOF\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx\n  labels:\n    tier: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      name: nginx\n      labels:\n        tier: frontend\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\nEOF\n# =======================================================================================\n02 `根据pod_nginx_rs.yml文件创建pod`\n\tkubectl apply -f pod_nginx_rs.yaml\n03 `查看pod`\n    kubectl get pods\n    kubectl get pods -o wide\n    kubectl describe pod nginx\n04 `感受通过rs将pod扩容`\n\tkubectl scale rs nginx --replicas=5\n\tkubectl get pods -o wide\n05 `删除pod`\n\tkubectl delete -f pod_nginx_rs.yaml\n```\n","tags":["k8s"],"categories":["devops"]},{"title":"SpringBoot自动装配机制","url":"/2019/12/14/SpringBoot自动装配机制/","content":"\nSpringBoot为什么会出现？说白了还是觉得Spring不够方便不够简化。SpringBoot一个脚手架让你一分钟就生成一个web项目\n\n为什么会这么快？为什么连基础配置都不需要？\n\n<!--more-->\n\n## SpringBoot自动装配如何实现的？\n\n### - SpringBoot\n\n> SpringBoot中的 @Configuration和@Bean 大家都很熟悉，标注此类是一个注解类，提示Spring扫描的作用。\n>\n> 那么如果我有这样一个诉求：\n>\n> 1. 我有100个@Configuration类\n> 2. 我某个模块需要让其中50个类 注入Spring中，该怎么操作？\n\n### - SpringBoot自动注入\n\n1. 从`@SpringBootApplication`入手，可以看见此注解是一个复合注解。其中`@SpringBootConfiguration`是继承`@Configuration`，所以这个注解我们可以知道他的大概作用，还有另一个注解\n\n   ```java\n   @Target(ElementType.TYPE)\n   @Retention(RetentionPolicy.RUNTIME)\n   @Documented\n   @Inherited\n   @SpringBootConfiguration\n   @EnableAutoConfiguration\n   @ComponentScan(excludeFilters = {\n   \t\t@Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),\n   \t\t@Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })\n   public @interface SpringBootApplication {}\n   ```\n\n   `@EnableAutoConfiguration`中有`@AutoConfigurationPackage`和`@Import`注解，`@AutoConfigurationPackage`注解是自动获取注册Package列表，下来我们看`@import(AutoConfigurationImportSelector.class)`的注解\n\n   ```java\n   @Target(ElementType.TYPE)\n   @Retention(RetentionPolicy.RUNTIME)\n   @Documented\n   @Inherited\n   @AutoConfigurationPackage\n   @Import(AutoConfigurationImportSelector.class)\n   public @interface EnableAutoConfiguration {\n   ```\n\n   > `@Import` 注解可以配置三种不同的 class\n   >\n   > 1. 基于普通 bean 或者带有`@Configuration` 的 `@bean` 进行注入\n   > 2. 实现 `ImportSelector` 接口进行动态注入\n   >\n   > ```java\n   > //通过实现selectImports方法，返回需要注入的ClassName即可实现动态注入。\n   > String[] selectImports(AnnotationMetadata importingClassMetadata);\n   > ```\n   >\n   > 3. 实现 `ImportBeanDefinitionRegistrar` 接口进行动态注入  \n   >\n   > ```java\n   > //通过实现registerBeanDefinitions方法，将需要返回的ClassName[]塞给registry即可实现动态注入\n   > void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry);\n   > ```\n   >\n\n   `AutoConfigurationImportSelector`这个类实现了`ImportSelector`类。\n\n2. 所以说实现`ImportSelector`就可以进行注入Spring，返回的`String[]`就是需要注入Spring的Bean的ClassName，那么我们可以想想，返回一个`String[]`数组的ClassNme，我们可不可以定义到一个配置文件中呢？配置文件我们可以这样配置 `key:ClassName[]`，key就是要满足的条件（一个注解类，只要注解了key注解类），即获取其中的`ClassName[]`，然后返回给Spring动态注入。这样不就方便管理了吗？\n\n   > SpringBoot就是这么操作的，SpringBoot定义了`spring.factories`文件，默认放在项目的`\\resources\\META-INF\\spring.factories`，SpringBoot加载所有项目下的spring.factories，将同一key的value[]整合，然后去重，然后去除`@exclude`注解过滤的类，剩余类全部注入。\n   >\n   > ==我们怎么使用？==\n   >\n   > 我们可以在自己的项目`\\resources\\META-INF\\`目录下建立`spring.factories`文件，按照SpringBoot中的格式。加入自己定义的`@Configuration`类即可实现Spring自动装配。\n   >\n   > ```properties\n   > org.springframework.boot.autoconfigure.EnableAutoConfiguration= \\\n   >   com.nephelo.commons.tool.redisconfig.RedissonAutoConfiguration,\\\n   >   com.nephelo.commons.tool.zookeeperConfig.ZookeeperAutoConfiguration\n   > ```\n   >\n   > 如果是自己定义的注解类(一般不建议)，我们需要实现`ImportBeanDefinitionRegistrar`或者`ImportSelector`在此注解上加上`@Import(AutoConfigurationImportSelector.class)`或者` import(ImportSelector.class)`即可。\n\n3. 自动装配的条件注解`@Conditional xxx`\n\n   > ```java\n   > @ConditionalOnClass  //当有某个class的时候加载\n   > @ConditionalOnBean   //当有某个bean的时候加载\n   > @ConditionalOnMissingClass  //当不存在某个class的时候加载\n   > @ConditionalOnMissingBean   //当不存在某个bean的时候加载\n   > ...\n   > ```\n\n+ 为什么？我们依赖一个`starter`就可以自动装配\n\n  > 就是因为Starter的jar包中，就会配置上面的自动装配装配，实现自动注入\n\n\n\n","tags":["SpringBoot"],"categories":["Spring"]},{"title":"MySQL的事务和ACID实现原理","url":"/2019/12/13/MySQL实现ACID原理/","content":"\n### ACID\n\nACID我们大家都知道，面试官也经常问到：\n\n不就是 原子性(Atomicity)、一致性(Consistency)、隔离性(Isolation)、持久性(Durability) 嘛！\n\n<!-- more -->\n\n#### 三大读取问题\n\n1. **脏读(P1)** ： 读取到别的事务修改的数据，但是别的事务未结束，也就是没有commit或者回滚。\n2. **不可重复读(P2)**：读取到别的事务修改的数据（或者删除的数据），但是别的事务已经commit。\n3. **幻读(P3)**：读取到别的事务增加的数据，并且已经提交。\n\n+ 其实都是读一致性问题\n\n#### 数据一致性标准（SQL92 ANSI/ISO）\n\n在http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt SQL92标准中，是如下定义的：\n\n1. **未提交读**(Read Uncommitted)：事务未提交的数据其他事务可见，解决P1\n2. **已提交读**(Read Committed)：事务只能读取别的事务提交后的数据，解决P2\n3. **可重复度**(Repeatable Read)：同一个事务中可以多次读取同样的数据 是一样的。\n   + SQL92官方未提出可以解决P3，但是在innodb中，不可能出现P3（==MySQL官网==）\n4. **串行化**(Serializable)：串行方式执行事务。解决P1P2P3\n\n### 两大实现方案\n\n如果要解决读一致问题，保证一个事务前后两次读取数据一致，该怎么做？在MySQL中用以下两种方案实现\n\n#### 1. LBCC\n\n**Lock Based Concurrency Control**：这种方案是基于锁的并发控制\n\n##### 1.1 锁的种类\n\n**行锁：**行级别的锁\n\n**表锁：**锁住一张表\n\n**共享锁：**也叫**读锁**，我们获取了 一行数据的读锁以后，可以用来读取数据，所以它也叫做读锁\n\n> ```sql\n> -- 我们可用以下方式加上读锁 \n> select ... lock in share mode;\n> ```\n>\n> 注意在加上了读锁以后是可以去写数据的，但是很可能会造成死锁。\n>\n> 释放锁有两种方式，只要事务结束，锁就会自动事务，包括提交事务和结束事务。\n\n**排它锁：**也叫**写锁**，只要一个事务获取了一行数据的排它锁，其他的事务就不能再获取这一行数据的共享锁和排它锁。 \n\n> ```sql\n> -- 我们可以加上FOR UPDATE的方式，给select加上排它锁\n> SELECT .... FOR UPDATE\n> ```\n\n**意向锁：**这个锁我们好像很陌生，也从来没用过？那这个锁是干什么的呢？其实它是由数据库自己维护，也就是说，当我们在给一行数据加上共享锁时，数据会自动给这张表加上意向共享锁；当我们在给一行数据加上排它锁时，数据会自动给这张表加上意向排它锁。\n\n反过来说： 如果一张表上面至少有一个意向共享锁，说明有其他的事务给其中的某些数据行加 上了共享锁。 如果一张表上面至少有一个意向排他锁，说明有其他的事务给其中的某些数据行加 上了排他锁。\n\n那这个锁有什么意义呢？\n\n> 1. 在我们给在某个表加上表锁时，他会怎么操作呢？他会首先扫描表中所有的行，如果某个行被加了行锁，那么就不允许在加表锁了，在大数据量时效率肯定会很低。如果有意向锁就不一样了，他就可以直接判断这个表是否有意向锁的存在，如果有那就说明有人使用行锁中。\n> 2. 这样以来，InnoDB就实现了表锁和行锁两个级别。虽然表锁只是一种标志的感觉。*注：MyISAM只支持表锁* \n\n##### 1.2 锁的原理\n\n一个锁他到底锁住了什么？我们知道Java锁会在对象头中标识，那么MySQL是不是也在头中呢？\n\n> 验证：\n>\n> 1. 我们先来看一下 t1 的表结构，它有两个字段，int 类型的 id 和 varchar 类型的 name。 里面有 4 条数据，1、2、3、4。\n>\n> | transaction1                             | transaction2                                            |\n> | ---------------------------------------- | ------------------------------------------------------- |\n> | begin;                                   |                                                         |\n> | SELECT * FROM t1 WHERE id =1 FOR UPDATE; |                                                         |\n> |                                          | select * from t1 where id=3 for update; --阻塞          |\n> |                                          | INSERT INTO `t1` (`id`, `name`) VALUES (5, '5'); --阻塞 |\n>\n> 为什么加了 id = 1的行锁， id=3和id=5的查询和插入都被阻塞了呢？难道是加了表锁？\n>\n> 2. 我们看一下 t2 的表结构。字段是一样的，不同的地方是 id 上创建了一个`主键索引`。 里面的数据是 1、4、7、10\n>\n> | transaction1                            | transaction2                                   |\n> | --------------------------------------- | ---------------------------------------------- |\n> | begin;                                  |                                                |\n> | select * from t2 where id=1 for update; |                                                |\n> |                                         | select * from t2 where id=1 for update; //阻塞 |\n> |                                         | select * from t2 where id=4 for update; // OK  |\n>\n> 可以看出使用不同`主键ID`是可以查询的\n\n> <font color='red'>结论：</font>\n>\n> 1）如果我们定义了主键(PRIMARY KEY)，那么 InnoDB 会选择主键作为聚集索引。 \n>\n> 2）如果没有显式定义主键，则 InnoDB 会选择第一个不包含有 NULL 值的唯一索 引作为主键索引。 \n>\n> 3）如果也没有这样的唯一索引，则 InnoDB 会选择内置 6 字节长的 ROWID 作为隐藏的聚集索引，它会随着行记录的写入而主键递增。\n>\n> 所以，为什么锁表，是因为查询没有使用索引，会进行全表扫描，然后把每一个隐 藏的聚集索引都锁住了。\n\n> 问题1：\n>\n> 为什么通过唯一索引加上行锁，主键索引也会被锁住？\n>\n> 在InnoDB中主键索引是聚集索引，唯一索引就是辅助索引。我们知道辅助索引的检索流程，在辅助索引的叶子节点存的是聚集索引的索引ID，所以还会再次检索聚集索引的树才能找到文件。所以当找到聚集索引之后肯定会锁起来。\n\n##### 1.3 锁的算法\n\n**记录锁**：\n\n当我们对于唯一性的索引（包括唯一索引和主键索引）使用等值查询，精准匹配到一条记录的时候，这个时候使用的就是记录锁。会只锁住当前的记录(record)\n\n**间隙锁：**\n\n当我们查询的记录不存在，没有命中任何一个 record，无论是用等值查询还是范围查询的时候，它使用的都是间隙锁。\n\n> 举个例子：我们的表中主键现在有 1 ，4，7，10，当我们执行`where id = 6`的时候，查询记录不存在，就会使用间隙锁。除了主键，外键和唯一性检查会加间隙锁。\n>\n> 关闭间隙锁：首先把事务级别设置为RC，并把 innodb_locks_unsafe_for_binlog 设置为 ON。\n\n**临键锁：**\n\n>  不仅仅命中了 Record 记录，还包含了 Gap间隙，在这种情况下我们使用的就是临键锁，它是 MySQL 里面默认的行锁算法，相当于记录锁加上间隙锁。唯一性索引，等值查询匹配到一条记录的时候，退化成记录锁\n\n> 举个例子：我们的表中主键现在有 1 ，4，7，10，当我们执行`where id > 5 and id < 9`的时候，查询记录就存在一个record 7\n>\n> ```sql\n> select * from t2 where id >5 and id <=7 for update; -- 锁住(4,7]和(7,10]\n> select * from t2 where id >8 and id <=10 for update; -- 锁住 (7,10]，(10,+∞)\n> ```\n>\n> 为什么要锁住下一个左开右闭的区间？——就是为了解决幻读的问题。 \n\n#### 2.MVCC\n\n**Multi Version Concurrency Control**：使用多版本的并发控制\n\n​\t**mvcc的实现原理**：\n\n+ 一个事务在一时间点读取，会把当前数据生成快照。\n\n+ innodb中快照会有两个标识，一个是（DB_TRX_ID）事务ID（==也可以理解为快照版本==），一个是（DB_ROLL_PTR）回滚指针。\n\n+ 事务1读取数据\n\n  > 读取数据，只会读取**DB_TRX_ID小于等于当前事务ID的数据和DB_ROLL_PTR大于当前事务ID**的数据。\n  >\n  > ```sql\n  > select * from user;\n  > ```\n\n  | ID   | name | DB_TRX_ID | DB_ROLL_PTR |\n  | ---- | ---- | --------- | ----------- |\n  | 1    | jack | 1         | undefined   |\n  | 2    | tom  | 1         | undefined   |\n\n  *-- 此时为初始状态*\n\n+ 事务2删除数据，会更新DB_ROLL_PTR的值为事务DB_TRX_ID+1\n\n  ```sql\n  //现在事务ID为2的事务 删除jack这个数据\n  delete user where id =1\n  ```\n\n  | ID   | name | DB_TRX_ID | DB_ROLL_PTR                |\n  | ---- | ---- | --------- | -------------------------- |\n  | 1    | jack | 1         | <font color='red'>2</font> |\n  | 2    | tom  | 1         | undefined                  |\n\n  *所以不影响 事务1 读取的数据*\n\n+ 事务3插入一条数据，此时快照中的变化。\n\n  > 插入数据会创建DB_TRX_ID为当前事务ID、DB_ROLL_PTR为undefined的数据。\n\n  ```sql\n  begin;\n  insert into user values (3,'jerry');\n  commit;\n  ```\n\n  | ID   | name  | DB_TRX_ID                   | DB_ROLL_PTR |\n  | ---- | ----- | --------------------------- | ----------- |\n  | 1    | jack  | 1                           | 2           |\n  | 2    | tom   | 1                           | undefined   |\n  | 3    | jerry | <font color='red'> 3</font> | undefined   |\n\n+ 事务1第二次查询\n\n  ```sql\n  select * from user;\n  ```\n\n  | ID   | name | DB_TRX_ID | DB_ROLL_PTR |\n  | ---- | ---- | --------- | ----------- |\n  | 1    | jack | 1         | 2           |\n  | 2    | tom  | 1         | undefined   |\n\n  **DB_TRX_ID小于等于当前事务ID的数据和DB_ROLL_PTR大于当前事务ID**\n\n+ 事务4更新一数据\n\n  > 更新数据会使快照创建出一条ID相同的更新后的数据，事务ID为删除事务的事务ID，DB_ROLL_PTR为null。并且更新源数据行的DB_ROLL_PTR为，更新操作事务ID+1\n\n  ```sql\n  //现在事务ID为3的事务更新了 jack这个数据\n  begin;\n  update user set name = 'pengyuyan' where id =1;\n  commit;\n  ```\n\n  | ID   | name      | DB_TRX_ID                  | DB_ROLL_PTR                |\n  | ---- | --------- | -------------------------- | -------------------------- |\n  | 1    | jack      | 1                          | <font color='red'>4</font> |\n  | 2    | tom       | 1                          | undefined                  |\n  | 3    | jerry     | 3                          | undefined                  |\n  | 1    | pengyuyan | <font color='red'>4</font> | undefined                  |\n\n  *也不影响 事务1读取的数据，现在的第一条数据叫做 undolog*\n\n#### 默认开启增删改事务\n\n```mysql\nshow global VARIABLES like 'autocommit'; //默认为ON的时候 增删改都会自动开启和关闭事务\n```\n\n### 总结\n\n#### 原子性的实现：\n\n通过`undo log`实现，`undo log`也就是数据修改之前的值，一但发生异常就可以通过`undo log`恢复。\n\n#### 一致性的实现：\n\n事务执行的 前后都是合法的数据状态。比如主键必须是唯一的，字段长度符合要求。但是除了数据提供的基础一致性，我们在操作时也要判断数据的合法性，比如：表中金额为20，操作时再减去100变成-80，这就不合法。\n\n#### 隔离性的实现：\n\n通过`mvcc`（基于快照）和`LBCC`（基于锁，表锁行锁）实现\n\n#### 持久性的实现：\n\n通过`redo log`（灾难恢复）和`double write`（页数据双写）双写缓冲来实现，`redo log `默认存放在`/var/lib/mysql/`目录中，默认一个48M\n\n```sql\nshow variables like 'innodb_log%'; 查看\ninnodb_log_file_size  # 指定每个文件的大小，默认 48M\ninnodb_log_files_in_group # 指定文件的数量，默认为 2\ninnodb_log_group_home_dir # 指定文件所在路径，相对或绝对。如果不指定，则为 datadir 路径。\n```\n\n","tags":["MySQL"],"categories":["Database"]},{"title":"Start","url":"/2019/01/01/Start/","content":"\n **不管怎么说，希望是一个美好的开始！！**\n\n<!-- more -->\n\n"}]